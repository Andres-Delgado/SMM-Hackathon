{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Mining - Hackathon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Popularity of Online Content\n",
    "Replicate and extend the article (link below) for predicting online content popularity using Twitter hashtags. Applying a method to a different dataset often entails making different choices from the ones of the original authors. Describe your experience and motivate your unique approach.\n",
    "\n",
    "### Dataset\n",
    "Using the Twitter Streaming API, you will collect a dataset of Twitter hashtags.\n",
    "\n",
    "### References\n",
    "- [Link to article](http://www.hpl.hp.com/research/scl/papers/predictions/predictions.pdf)\n",
    "- [Twitter Streaming API](https://developer.twitter.com/en/docs/tweets/sample-realtime/overview/GET_statuse_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import json\n",
    "import os\n",
    "\n",
    "CONSUMER_KEY = 'n3klbsDYyvvk1IxCEvm7W3tGw'\n",
    "CONSUMER_SECRET ='CJZ71dbUlhjDu7DyzawOCBMPkltdpw6816WZ3WUgC8rbOcKSs8'\n",
    "OAUTH_TOKEN = '1031697780417417216-TwltGeXHoG0eybrj8tF1AAckCj2bDb'\n",
    "OAUTH_TOKEN_SECRET = 'skAVmJwY1BhbrPWAeJB5uOWMacpikpYw2n8XAXnUI3TwU'\n",
    "\n",
    "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "twitter_api = twitter.Twitter(auth=auth)\n",
    "tStream = twitter.TwitterStream(auth = auth)\n",
    "tSample = tStream.statuses.sample() # stream sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NameFile() creates a file name to make sure we don't overwite previous sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NameFile():\n",
    "    pathName = os.path.join('Data', 'Tweets')\n",
    "    \n",
    "    if not os.path.isfile(pathName + '.json'):\n",
    "        return pathName + '.json'\n",
    "    \n",
    "    count = 1\n",
    "    while (os.path.isfile(pathName + str(count) + '.json')):\n",
    "        count += 1\n",
    "            \n",
    "    return pathName + str(count) + '.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StoreTweets() writes sampled tweets to ../Data/Tweets.json\n",
    "Filters out deleted tweets and tweets without hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StoreTweets(pathToTweets, tSample):\n",
    "    with open(os.path.join(pathToTweets), 'w') as f:\n",
    "        f.write('[')\n",
    "\n",
    "        count = 0\n",
    "        for tweet in tSample:\n",
    "            #if (list(tweet.keys())[0] != 'delete') and (tweet.get('entities', False)):\n",
    "            if (tweet.get('delete', False) is False) and (tweet.get('entities', False)):\n",
    "                if tweet['entities'].get('hashtags', False):\n",
    "                    count += 1\n",
    "                    json.dump(tweet, f)\n",
    "                    if count < 100: f.write(',\\n')\n",
    "                    else: break\n",
    "        f.write(']')\n",
    "        f.close()\n",
    "        print(count, \"tweets written to file\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathToTweets = NameFile()\n",
    "#StoreTweets(pathToTweets, tSample)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoadFile() opens the specified file and returns a json object of its contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadFile(pathName):    \n",
    "    if os.path.isfile(pathName):\n",
    "        file = open(os.path.join(pathName), 'r')\n",
    "        return json.load(file)\n",
    "    else: print(\"File not in directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = LoadFile(os.path.join('Data', 'Tweets.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StoreHashtags() extracts all unique hashtags from LoadTweets() and writes them to Tweets(#)-Hashtags.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StoreHashtags(pathName, givenTweets):\n",
    "    hashtagList = []\n",
    "    for tweet in givenTweets:\n",
    "        for hTag in tweet['entities']['hashtags']:\n",
    "            hashtagList.append(hTag['text'])\n",
    "        \n",
    "    hashtagList = list(set(hashtagList))\n",
    "    newPath = pathName[:-5] + \"-Hashtags.json\"\n",
    "\n",
    "    with open(newPath, 'w') as f:    \n",
    "        json.dump(hashtagList, f)\n",
    "        f.close()\n",
    "    print(\"Stored all unique hashtags in {}\".format(newPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StoreHashtags(os.path.join('Data', 'Tweets.json'), tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hashtagList = LoadFile(os.path.join('Data', 'Tweets-Hashtags.json'))\n",
    "#print(json.dumps(hashtags, indent = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDVX\n",
      "ARIAsCamilaCabello\n",
      "EXILETRIBE\n",
      "liseli\n",
      "EEUU\n",
      "sarapngdaddy\n",
      "LEEKNOW\n"
     ]
    }
   ],
   "source": [
    "for i, tag in enumerate(hashtagList):\n",
    "    print(tag)\n",
    "    if i > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### StoreDict() takes in the list of hashtag strings (hashtagList), constructs a dictionary of hashtag dictionaries, writes them to Tweets(#)-Hashtags-Dict.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StoreDictionary(pathName, givenHTList, numberOfDays):\n",
    "    # use of other hashtags, media(type?), user_mentions, filter_level\n",
    "    details = {'occurrences': 0, 'created_at': [], 'followers_count': [], 'lang': {}}\n",
    "    \n",
    "    days = {}    \n",
    "    for i in range(numberOfDays):\n",
    "        days.setdefault('day' + str(i), dict(details))\n",
    "\n",
    "    hashtagsDict = {}\n",
    "    for i, tag in enumerate(givenHTList):\n",
    "        hashtagsDict.setdefault(tag, dict(days))\n",
    "    \n",
    "    newPath = pathName[:-5] + \"-Dict.json\"\n",
    "    with open(newPath, 'w') as f:    \n",
    "        json.dump(hashtagsDict, f)\n",
    "        f.close()\n",
    "    print(\"Stored the hashtag dictionary in {}\".format(newPath))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StoreDictionary(os.path.join('Data', 'Tweets-Hashtags.json'), hashtagList, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = LoadFile(os.path.join('Data', 'Tweets-Hashtags-Dict.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hashtags: 5289\n",
      "\n",
      "Hashtag: SDVX\n",
      "{\n",
      " \"day0\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day1\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day2\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day3\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day4\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day5\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day6\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day7\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day8\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day9\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day10\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day11\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day12\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day13\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day14\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day15\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day16\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day17\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day18\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day19\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day20\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day21\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day22\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day23\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day24\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day25\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day26\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day27\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day28\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " },\n",
      " \"day29\": {\n",
      "  \"occurrences\": 0,\n",
      "  \"created_at\": [],\n",
      "  \"followers_count\": [],\n",
      "  \"lang\": {}\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique hashtags:\", len(hashtags), end = '\\n\\n')\n",
    "for tag, content in hashtags.items():\n",
    "    print(\"Hashtag:\", tag)\n",
    "    print(json.dumps(content, indent = 1))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtag Dictionary: \n",
    "- **keys:** hashtag string\n",
    "- **values:** dictionary of day objects\n",
    "\n",
    "### day objects:\n",
    "- **occurences:** number of occurences of hashtag for given day\n",
    "- **created_at:** list (strings) of creation times of all (or some?) tweets using hashtag\n",
    "- **followers_count:** list (ints) of number followers the user has who posted the tweet\n",
    "- **lang:** dictionary to count the number of self-declared languages by user\n",
    "    - \"en\": 100\n",
    "    - \"es\": 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
